{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vyelizarov/anaconda3/envs/109b4/lib/python3.6/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import datetime\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import tables\n",
    "import io\n",
    "from zipfile import ZipFile\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import gensim as gensim\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from itertools import chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Attempts to group song lyrics into topics and matching hot songs' words back to songs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the Million Song Dataset subset (uncompressed) - change to the location on your laptop\n",
    "# Cannot store this on github as it is too large\n",
    "msd_subset_path = '../../MSD_data/MillionSongSubset/'\n",
    "\n",
    "# Keep these - folders match the structure of the uncompressed file\n",
    "msd_subset_data_path = os.path.join(msd_subset_path, 'data')\n",
    "msd_subset_addf_path = os.path.join(msd_subset_path, 'AdditionalFiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses CountVectorizer to extract words from texts\n",
    "def getCntVect(what, stopwords) :\n",
    "    count_vect = CountVectorizer(input='fname',\n",
    "                                 stop_words=stopwords)\n",
    "    dataset = count_vect.fit_transform(what)\n",
    "    return (count_vect, dataset)\n",
    "\n",
    "# Parses MXM file (from MSD website) and returns track - reconstructed song text (obviously, without order)\n",
    "def getText(fname, check_id = None) :\n",
    "    track2i = {}\n",
    "    i = 0\n",
    "    allStr = []\n",
    "    text = []\n",
    "    with ZipFile('../../MSD_data/' + fname + '.zip') as z:\n",
    "        with z.open(fname) as src:\n",
    "            for line in src:\n",
    "                line = line.decode(\"utf-8\")\n",
    "                if line[0] == '#' :\n",
    "                    continue\n",
    "                if line[0] == '%' :\n",
    "                    all_words = line[1:].split(',')\n",
    "                    continue\n",
    "                parts = line.split(',')\n",
    "                trackid = parts[0]\n",
    "                if check_id != None and trackid not in check_id :\n",
    "                    continue\n",
    "                track2i[trackid] = i\n",
    "                i += 1\n",
    "                str = ''\n",
    "                strOnly = set()\n",
    "                for i in range(2, len(parts)) :\n",
    "                    code = parts[i].split(':')\n",
    "                    w = all_words[int(code[0]) - 1]\n",
    "                    strOnly.add(w)\n",
    "                    for j in range(int(code[1])) :\n",
    "                        str += w\n",
    "                        str += ' '\n",
    "                text.append(str)\n",
    "                allStr.append(strOnly)\n",
    "\n",
    "    return (track2i, text, all_words, allStr)\n",
    "\n",
    "(track2i, text, all_words, allStr) = getText('mxm_dataset.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs LdaModel on MXM texts\n",
    "def doLda(text) :\n",
    "    (cnt, data) = getCntVect(text, [])\n",
    "    corpus = gensim.matutils.Sparse2Corpus(data, documents_columns=False)\n",
    "    id_map = dict((v, k) for k, v in cnt.vocabulary_.items())\n",
    "    k = 4\n",
    "    model = LdaModel(corpus, num_topics=k, id2word = id_map, passes=10)\n",
    "    top_words = [[word for word,_ in model.show_topic(topicno, topn=50)] for topicno in range(model.num_topics)]\n",
    "    top_betas = [[beta for _,beta in model.show_topic(topicno, topn=50)] for topicno in range(model.num_topics)]\n",
    "    topn = 10\n",
    "    for t in range(k) :\n",
    "        print(\"Topic\", t + 1)\n",
    "        for w, b in zip(top_words[t][:topn], top_betas[t][:topn]) :\n",
    "            print(\"\\t\", w, b)\n",
    "    return (model, corpus, id_map, top_words, top_betas, topn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1\n",
      "\t you 0.06896217\n",
      "\t not 0.02772753\n",
      "\t to 0.027723389\n",
      "\t it 0.02626611\n",
      "\t me 0.026186557\n",
      "\t and 0.01999828\n",
      "\t do 0.018791465\n",
      "\t the 0.016847981\n",
      "\t my 0.015083564\n",
      "\t that 0.013842474\n",
      "Topic 2\n",
      "\t ich 0.039700765\n",
      "\t und 0.031753503\n",
      "\t die 0.029994661\n",
      "\t du 0.025936678\n",
      "\t da 0.023506653\n",
      "\t der 0.01786892\n",
      "\t nicht 0.016851075\n",
      "\t das 0.015330857\n",
      "\t ist 0.014606803\n",
      "\t in 0.014363281\n",
      "Topic 3\n",
      "\t the 0.084027715\n",
      "\t and 0.03257721\n",
      "\t of 0.02437285\n",
      "\t in 0.023455951\n",
      "\t to 0.022142988\n",
      "\t is 0.015319636\n",
      "\t my 0.013463411\n",
      "\t we 0.0129176825\n",
      "\t on 0.012376971\n",
      "\t it 0.009642142\n",
      "Topic 4\n",
      "\t la 0.039082862\n",
      "\t que 0.038148064\n",
      "\t de 0.036884256\n",
      "\t no 0.02104369\n",
      "\t me 0.019135416\n",
      "\t en 0.017834427\n",
      "\t el 0.015212119\n",
      "\t un 0.014472179\n",
      "\t te 0.014052592\n",
      "\t tu 0.013633844\n"
     ]
    }
   ],
   "source": [
    "(model, corpus, id_map, top_words, top_betas, topn) = doLda(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_full = pd.read_pickle(msd_subset_path+'subset_full_clean.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For classification\n",
    "# Simplify by only setting top 25% of song_hotness to be \"hot\" and the rest \"not\"\n",
    "\n",
    "def convert_y_to_categorical(cutoff = 0.75):\n",
    "    threshold = y.quantile(cutoff)\n",
    "    Y = [0 if i < threshold else 1 for i in y]\n",
    "    return np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do LDA for only hit songs\n",
    "y = subset_full['song_hotttnesss']\n",
    "y = convert_y_to_categorical()\n",
    "track_hit = subset_full[y == 1].track_id\n",
    "tr_hit_set = set()\n",
    "for tr in track_hit :\n",
    "    tr_hit_set.add(tr)\n",
    "(track2iH, textH, all_wordsH, allStrH) = getText('mxm_dataset.txt', tr_hit_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1\n",
      "\t que 0.04219505\n",
      "\t la 0.022654649\n",
      "\t de 0.022192998\n",
      "\t me 0.016727462\n",
      "\t no 0.015936652\n",
      "\t en 0.015661983\n",
      "\t el 0.014473983\n",
      "\t tu 0.013031961\n",
      "\t mi 0.013021611\n",
      "\t te 0.011659139\n",
      "Topic 2\n",
      "\t you 0.046358056\n",
      "\t and 0.03077111\n",
      "\t not 0.018667452\n",
      "\t it 0.017111741\n",
      "\t the 0.016676141\n",
      "\t to 0.01463232\n",
      "\t is 0.013307138\n",
      "\t my 0.011992037\n",
      "\t have 0.011016317\n",
      "\t do 0.010618285\n",
      "Topic 3\n",
      "\t the 0.07309293\n",
      "\t and 0.026463933\n",
      "\t of 0.021253124\n",
      "\t to 0.02118038\n",
      "\t we 0.019077085\n",
      "\t it 0.017731711\n",
      "\t in 0.017420586\n",
      "\t is 0.015665404\n",
      "\t your 0.014126421\n",
      "\t you 0.013375721\n",
      "Topic 4\n",
      "\t you 0.05128066\n",
      "\t the 0.031130154\n",
      "\t to 0.026808992\n",
      "\t it 0.024223857\n",
      "\t me 0.021972094\n",
      "\t and 0.021395301\n",
      "\t not 0.019841786\n",
      "\t my 0.017942755\n",
      "\t is 0.0146789\n",
      "\t am 0.012874113\n"
     ]
    }
   ],
   "source": [
    "(modelH, corpusH, id_mapH, top_wordsH, top_betasH, topnH) = doLda(textH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in hot songs: {'believ', 'solo', 'whi', 'danc', 'away', 'more', 'see', 'stop', 'tanto', 'start', 'dos', 'think', 'mas', 'cada', 'todo', 'eu', 'nos', 'again', 'hay', 'round', 'parti', 'better', 'nÃ£o', 'ven', 'would'}\n"
     ]
    }
   ],
   "source": [
    "# Extract words which are present in topics of hit songs but not all songs\n",
    "allW = set(chain.from_iterable(top_words))\n",
    "hotW = set(chain.from_iterable(top_wordsH))\n",
    "keyWords = hotW - allW\n",
    "print('Words in hot songs:', keyWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure subset_full only has tracks we have lyrics for\n",
    "track_ids = subset_full.values[:, 0]\n",
    "for i in range(subset_full.shape[0]) :\n",
    "    track = track_ids[i]\n",
    "    if track not in track2i :\n",
    "        subset_full.drop(subset_full[subset_full.track_id == track].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict a track to be hit if it contains key words of hit songs\n",
    "y_pred = np.zeros(subset_full.shape[0])\n",
    "track_ids = subset_full.values[:, 0]\n",
    "c = 0\n",
    "for i in range(subset_full.shape[0]) :\n",
    "    track = track_ids[i]\n",
    "    if track in track2i :\n",
    "        j = track2i[track]\n",
    "        for w in keyWords :\n",
    "            if w in allStr[j] :\n",
    "                y_pred[i] = 1\n",
    "    else :\n",
    "        print ('track not found', track)\n",
    "        c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = subset_full['song_hotttnesss']\n",
    "y = convert_y_to_categorical()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that nothing is skipped - set was already cleaned \n",
    "assert c == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.39148681055155876\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy', accuracy_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*~40% accuracy - much lower than baseline*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
