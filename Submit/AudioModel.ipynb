{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "span.sub-q {\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A; \t \n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import datetime\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import tables\n",
    "import io\n",
    "import gzip\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras import backend as K,objectives\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import model_from_json\n",
    "from sklearn.metrics import classification_report, confusion_matrix, mean_squared_error, accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the Million Song Dataset subset (uncompressed) - change to the location on your laptop\n",
    "# Cannot store this on github as it is too large\n",
    "msd_subset_path = '../../MSD_data/MillionSongSubset/'\n",
    "\n",
    "# Keep these - folders match the structure of the uncompressed file\n",
    "msd_subset_data_path = os.path.join(msd_subset_path, 'data')\n",
    "msd_subset_addf_path = os.path.join(msd_subset_path, 'AdditionalFiles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Use Rhythm Histogram and Statistical Spectrum Descriptor data from http://www.ifs.tuwien.ac.at/mir/audiofeatureextraction.html to attempt to predict hotness*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rh_hotness_clean = pd.read_pickle(msd_subset_path+'rh_and_hotness_clean.pkl')\n",
    "ssd_hotness_clean = pd.read_pickle(msd_subset_path+'ssd_and_hotness_clean.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Check for duplicate tracks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (ssd_hotness_clean.groupby('song_id').count()['track_id'] > 1).sum() == 0 \n",
    "assert (rh_hotness_clean.groupby('song_id').count()['track_id'] > 1).sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For classification\n",
    "# Simplify by only setting top 25% of song_hotness to be \"hot\" and the rest \"not\"\n",
    "\n",
    "def convert_y_to_categorical(y_df, cutoff = 0.75):\n",
    "    threshold = y_df.quantile(cutoff)\n",
    "    Y = [0 if i < threshold else 1 for i in y_df]\n",
    "    return np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outline taken from https://www.pyimagesearch.com/2019/01/28/keras-regression-and-cnns/\n",
    "\n",
    "def create_cnn(width, height, depth, filters=(16, 32, 64), regress=True):\n",
    "    # initialize the input shape and channel dimension, assuming\n",
    "    # TensorFlow/channels-last ordering\n",
    "    inputShape = (width, height, depth)\n",
    "    chanDim = -1\n",
    "    # define the model input\n",
    "    inputs = Input(shape=inputShape)\n",
    "    # loop over the number of filters\n",
    "    for (i, f) in enumerate(filters):\n",
    "    # if this is the first CONV layer then set the input appropriately\n",
    "        if i == 0:\n",
    "            x = inputs\n",
    "        x = Conv2D(f, (3, 3), padding=\"same\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = BatchNormalization(axis=chanDim)(x)\n",
    "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    # flatten the volume, then FC => RELU => BN => DROPOUT\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(16)(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization(axis=chanDim)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    # apply another FC layer, this one to match the number of nodes coming out of the MLP\n",
    "    x = Dense(4)(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    # check to see if the regression node should be added\n",
    "    if regress:\n",
    "        x = Dense(1, activation=\"linear\")(x)\n",
    "    else :\n",
    "        x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    # construct the CNN\n",
    "    model = Model(inputs, x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Combine features into single dataset for training*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = rh_hotness_clean.song_hotttnesss\n",
    "x_rh = rh_hotness_clean.drop(['track_id', 'song_id', 'song_hotttnesss'], axis = 1)\n",
    "x_ssd = ssd_hotness_clean.drop(['track_id', 'song_id', 'song_hotttnesss'], axis = 1)\n",
    "x = np.concatenate((x_rh, x_ssd), axis = 1)\n",
    "# df = ssd_hotness_clean # rh_hotness_clean\n",
    "# x = df.drop(['track_id', 'song_id', 'song_hotttnesss'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_rh = 12 # 21 15\n",
    "h_rh = 19 # 8  4\n",
    "\n",
    "x = MinMaxScaler().fit_transform(x)\n",
    "y_df = y\n",
    "y = y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModelAndDs(x, y, w_rh, h_rh, regress) :\n",
    "    (trainX, testX, trainY, testY) = train_test_split(x, y, test_size=0.25, random_state=109)\n",
    "    trainX = np.expand_dims(trainX.reshape(trainX.shape[0], w_rh, h_rh), axis = -1)\n",
    "    testX = np.expand_dims(testX.reshape(testX.shape[0], w_rh, h_rh), axis = -1)\n",
    "    K.clear_session()\n",
    "\n",
    "    model = create_cnn(w_rh, h_rh, 1, regress=regress)\n",
    "    opt = Adam(lr=1e-3, decay=1e-3 / 200)\n",
    "    model.compile(loss=\"mean_absolute_percentage_error\", optimizer=opt, metrics = ['accuracy'])\n",
    "    model.summary()\n",
    "    return (model, trainX, testX, trainY, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, trainX, testX, trainY, testY, regress) :\n",
    "    csv_logger = CSVLogger('history_regress{}.log'.format(regress), separator=',', append=False)\n",
    "    history = model.fit(trainX, trainY, validation_data=(testX, testY), epochs=200, batch_size=8,\n",
    "                                     callbacks=[csv_logger]);\n",
    "    model.save_weights(\"audio_model_regress{}.h5\".format(regress))\n",
    "    model_json = model.to_json()\n",
    "    with open(\"audio_model_regress{}.json\".format(regress), \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    loss, accuracy  = model.evaluate(testX, testY, verbose=False)\n",
    "    return (model, history, loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(history, loss, accuracy) :\n",
    "    print(f'Test loss: {loss:.3}')\n",
    "    print(f'Test accuracy: {accuracy:.3}')\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(5,5))\n",
    "    ax.plot((history.history['acc']), 'r', label='train')\n",
    "    ax.plot((history.history['val_acc']), 'b' ,label='val')\n",
    "    ax.set_xlabel(r'Epoch', fontsize=20)\n",
    "    ax.set_ylabel(r'Accuracy', fontsize=20)\n",
    "    ax.legend()\n",
    "    ax.tick_params(labelsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 12, 19, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 12, 19, 16)        160       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 12, 19, 16)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 12, 19, 16)        64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 9, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 6, 9, 32)          4640      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 6, 9, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 6, 9, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 3, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 4, 64)          18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 3, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 3, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 1, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                2064      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 68        \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 25,945\n",
      "Trainable params: 25,689\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "regress = True\n",
    "model, trainX, testX, trainY, testY = getModelAndDs(x, y, w_rh, h_rh, regress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4236 samples, validate on 1412 samples\n",
      "Epoch 1/200\n",
      "4236/4236 [==============================] - 8s 2ms/step - loss: 6125932.1479 - acc: 0.2493 - val_loss: 237178262.4767 - val_acc: 0.0000e+00\n",
      "Epoch 2/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 68070.4276 - acc: 0.2524 - val_loss: 95700.2355 - val_acc: 0.2585\n",
      "Epoch 3/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 65061.0896 - acc: 0.2524 - val_loss: 72430.3399 - val_acc: 0.2585\n",
      "Epoch 4/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 66608.2025 - acc: 0.2524 - val_loss: 114518.0179 - val_acc: 0.2585\n",
      "Epoch 5/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 65089.8691 - acc: 0.2524 - val_loss: 74343.1729 - val_acc: 0.2585\n",
      "Epoch 6/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 61717.4223 - acc: 0.2524 - val_loss: 2638.8682 - val_acc: 0.2585\n",
      "Epoch 7/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 75831.0522 - acc: 0.2524 - val_loss: 82369.7561 - val_acc: 0.2585\n",
      "Epoch 8/200\n",
      "4236/4236 [==============================] - 6s 2ms/step - loss: 52410.2226 - acc: 0.2524 - val_loss: 5643.5148 - val_acc: 0.2585\n",
      "Epoch 9/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 53610.3341 - acc: 0.2524 - val_loss: 23585.1105 - val_acc: 0.2585\n",
      "Epoch 10/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 56349.2503 - acc: 0.2524 - val_loss: 57030.3341 - val_acc: 0.2585\n",
      "Epoch 11/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 53535.7201 - acc: 0.2524 - val_loss: 29293.1386 - val_acc: 0.2585\n",
      "Epoch 12/200\n",
      "4236/4236 [==============================] - 6s 2ms/step - loss: 49662.5719 - acc: 0.2524 - val_loss: 31686.6987 - val_acc: 0.2585\n",
      "Epoch 13/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 60235.8917 - acc: 0.2524 - val_loss: 8583.9327 - val_acc: 0.2585\n",
      "Epoch 14/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 64107.0340 - acc: 0.2524 - val_loss: 94374.6554 - val_acc: 0.2585\n",
      "Epoch 15/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 51525.3161 - acc: 0.2524 - val_loss: 38554.3821 - val_acc: 0.2585\n",
      "Epoch 16/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 53842.7109 - acc: 0.2524 - val_loss: 188970.3187 - val_acc: 0.2585\n",
      "Epoch 17/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 57155.4194 - acc: 0.2524 - val_loss: 52272.1362 - val_acc: 0.2585\n",
      "Epoch 18/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 64213.2788 - acc: 0.2524 - val_loss: 32172.1687 - val_acc: 0.2585\n",
      "Epoch 19/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 68699.6419 - acc: 0.2524 - val_loss: 345.3602 - val_acc: 0.2585\n",
      "Epoch 20/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 45771.2921 - acc: 0.2524 - val_loss: 72564.0802 - val_acc: 0.2585\n",
      "Epoch 21/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 71167.4157 - acc: 0.2524 - val_loss: 14277.2641 - val_acc: 0.2585\n",
      "Epoch 22/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 47514.3206 - acc: 0.2524 - val_loss: 79344.0585 - val_acc: 0.2585\n",
      "Epoch 23/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 64445.8160 - acc: 0.2524 - val_loss: 11593.9970 - val_acc: 0.2585\n",
      "Epoch 24/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 58909.3801 - acc: 0.2524 - val_loss: 35101.3092 - val_acc: 0.2585\n",
      "Epoch 25/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 56643.7832 - acc: 0.2524 - val_loss: 29913.3814 - val_acc: 0.2585\n",
      "Epoch 26/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 51932.1812 - acc: 0.2524 - val_loss: 7727.8786 - val_acc: 0.2585\n",
      "Epoch 27/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 68077.5271 - acc: 0.2524 - val_loss: 5986.7375 - val_acc: 0.2585\n",
      "Epoch 28/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 59881.5452 - acc: 0.2524 - val_loss: 140589.8724 - val_acc: 0.2585\n",
      "Epoch 29/200\n",
      "4236/4236 [==============================] - 6s 1ms/step - loss: 57398.1980 - acc: 0.2524 - val_loss: 26946.4965 - val_acc: 0.2585\n",
      "Epoch 30/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 59220.2947 - acc: 0.2524 - val_loss: 10101.7048 - val_acc: 0.2585\n",
      "Epoch 31/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 55895.0866 - acc: 0.2524 - val_loss: 124557.2381 - val_acc: 0.2585\n",
      "Epoch 32/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 61818.8268 - acc: 0.2524 - val_loss: 49671.3157 - val_acc: 0.2585\n",
      "Epoch 33/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 62589.1474 - acc: 0.2524 - val_loss: 38761.9324 - val_acc: 0.2585\n",
      "Epoch 34/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 62284.0988 - acc: 0.2524 - val_loss: 53981.4304 - val_acc: 0.2585\n",
      "Epoch 35/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 85216.0378 - acc: 0.2524 - val_loss: 77058.3575 - val_acc: 0.2585\n",
      "Epoch 36/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 57461.3515 - acc: 0.2524 - val_loss: 129527.6446 - val_acc: 0.2585\n",
      "Epoch 37/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 50340.1281 - acc: 0.2524 - val_loss: 3696.6514 - val_acc: 0.2585\n",
      "Epoch 38/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 57001.3736 - acc: 0.2524 - val_loss: 17196.3512 - val_acc: 0.2585\n",
      "Epoch 39/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 52720.7987 - acc: 0.2524 - val_loss: 26378.7592 - val_acc: 0.2585\n",
      "Epoch 40/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 69859.2176 - acc: 0.2524 - val_loss: 9726.1925 - val_acc: 0.2585\n",
      "Epoch 41/200\n",
      "4236/4236 [==============================] - 6s 2ms/step - loss: 54915.3490 - acc: 0.2524 - val_loss: 69083.4663 - val_acc: 0.2585\n",
      "Epoch 42/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 50478.2731 - acc: 0.2524 - val_loss: 150660.2025 - val_acc: 0.2585\n",
      "Epoch 43/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 63741.8599 - acc: 0.2524 - val_loss: 1757.9422 - val_acc: 0.2585\n",
      "Epoch 44/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 41316.8487 - acc: 0.2524 - val_loss: 52440.7152 - val_acc: 0.2585\n",
      "Epoch 45/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 53209.4454 - acc: 0.2524 - val_loss: 21738.4464 - val_acc: 0.2585\n",
      "Epoch 46/200\n",
      "4236/4236 [==============================] - 6s 1ms/step - loss: 73140.9954 - acc: 0.2524 - val_loss: 60358.0075 - val_acc: 0.2585\n",
      "Epoch 47/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 63796.0106 - acc: 0.2524 - val_loss: 38446.3267 - val_acc: 0.2585\n",
      "Epoch 48/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 62948.5764 - acc: 0.2524 - val_loss: 30992.7234 - val_acc: 0.2585\n",
      "Epoch 49/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 47988.3936 - acc: 0.2524 - val_loss: 17741.2687 - val_acc: 0.2585\n",
      "Epoch 50/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 55560.8607 - acc: 0.2524 - val_loss: 64075.6263 - val_acc: 0.2585\n",
      "Epoch 51/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 63507.9373 - acc: 0.2524 - val_loss: 69657.6244 - val_acc: 0.2585\n",
      "Epoch 52/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 48567.1408 - acc: 0.2524 - val_loss: 14318.1944 - val_acc: 0.2585\n",
      "Epoch 53/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 56741.6861 - acc: 0.2524 - val_loss: 83528.1606 - val_acc: 0.2585\n",
      "Epoch 54/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 49302.5679 - acc: 0.2524 - val_loss: 30258.5588 - val_acc: 0.2585\n",
      "Epoch 55/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 42148.0701 - acc: 0.2524 - val_loss: 58029.3483 - val_acc: 0.2585\n",
      "Epoch 56/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 42334.4257 - acc: 0.2524 - val_loss: 26064.6949 - val_acc: 0.2585\n",
      "Epoch 57/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4236/4236 [==============================] - 7s 2ms/step - loss: 51028.1977 - acc: 0.2524 - val_loss: 118966.8934 - val_acc: 0.2585\n",
      "Epoch 58/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 58641.5086 - acc: 0.2524 - val_loss: 64038.3658 - val_acc: 0.2585\n",
      "Epoch 59/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 54585.8324 - acc: 0.2524 - val_loss: 57311.0592 - val_acc: 0.2585\n",
      "Epoch 60/200\n",
      "4236/4236 [==============================] - 6s 2ms/step - loss: 56219.8803 - acc: 0.2524 - val_loss: 4815.2347 - val_acc: 0.2585\n",
      "Epoch 61/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 55997.6809 - acc: 0.2524 - val_loss: 43698.2994 - val_acc: 0.2585\n",
      "Epoch 62/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 50546.4644 - acc: 0.2524 - val_loss: 39001.4760 - val_acc: 0.2585\n",
      "Epoch 63/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 65166.8250 - acc: 0.2524 - val_loss: 28259.2342 - val_acc: 0.2585\n",
      "Epoch 64/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 64761.7507 - acc: 0.2524 - val_loss: 57651.9094 - val_acc: 0.2585\n",
      "Epoch 65/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 50473.2607 - acc: 0.2524 - val_loss: 94715.9464 - val_acc: 0.2585\n",
      "Epoch 66/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 58807.4021 - acc: 0.2524 - val_loss: 186911.3730 - val_acc: 0.2585\n",
      "Epoch 67/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 46856.9089 - acc: 0.2524 - val_loss: 71551.2032 - val_acc: 0.2585\n",
      "Epoch 68/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 48905.6196 - acc: 0.2524 - val_loss: 14243.9064 - val_acc: 0.2585\n",
      "Epoch 69/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 46490.0945 - acc: 0.2524 - val_loss: 32426.8930 - val_acc: 0.2585\n",
      "Epoch 70/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 38423.5202 - acc: 0.2524 - val_loss: 41111.8568 - val_acc: 0.2585\n",
      "Epoch 71/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 45839.8588 - acc: 0.2524 - val_loss: 21007.3544 - val_acc: 0.2585\n",
      "Epoch 72/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 52240.7340 - acc: 0.2524 - val_loss: 8653.2710 - val_acc: 0.2585\n",
      "Epoch 73/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 41544.0538 - acc: 0.2524 - val_loss: 83761.4715 - val_acc: 0.2585\n",
      "Epoch 74/200\n",
      "4236/4236 [==============================] - 5s 1ms/step - loss: 38539.8758 - acc: 0.2524 - val_loss: 1304.5038 - val_acc: 0.2585\n",
      "Epoch 75/200\n",
      "4236/4236 [==============================] - 6s 1ms/step - loss: 45511.3058 - acc: 0.2524 - val_loss: 23787.0886 - val_acc: 0.2585\n",
      "Epoch 76/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 62310.7465 - acc: 0.2524 - val_loss: 18935.6241 - val_acc: 0.2585\n",
      "Epoch 77/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 44649.2725 - acc: 0.2524 - val_loss: 40231.4598 - val_acc: 0.2585\n",
      "Epoch 78/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 56401.9545 - acc: 0.2524 - val_loss: 83692.1372 - val_acc: 0.2585\n",
      "Epoch 79/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 46609.6432 - acc: 0.2524 - val_loss: 7736.4304 - val_acc: 0.2585\n",
      "Epoch 80/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 45970.1970 - acc: 0.2524 - val_loss: 94401.9425 - val_acc: 0.2585\n",
      "Epoch 81/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 49393.5465 - acc: 0.2524 - val_loss: 54497.1804 - val_acc: 0.2585\n",
      "Epoch 82/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 47471.4046 - acc: 0.2524 - val_loss: 22419.1716 - val_acc: 0.2585\n",
      "Epoch 83/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 44961.6213 - acc: 0.2524 - val_loss: 49315.0897 - val_acc: 0.2585\n",
      "Epoch 84/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 42590.1765 - acc: 0.2524 - val_loss: 75385.0128 - val_acc: 0.2585\n",
      "Epoch 85/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 46661.1287 - acc: 0.2524 - val_loss: 28314.1225 - val_acc: 0.2585\n",
      "Epoch 86/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 48399.3779 - acc: 0.2524 - val_loss: 31042.9027 - val_acc: 0.2585\n",
      "Epoch 87/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 51973.9139 - acc: 0.2524 - val_loss: 70570.4037 - val_acc: 0.2585\n",
      "Epoch 88/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 66307.2459 - acc: 0.2524 - val_loss: 22492.8861 - val_acc: 0.2585\n",
      "Epoch 89/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 45969.3002 - acc: 0.2524 - val_loss: 117959.2101 - val_acc: 0.2585\n",
      "Epoch 90/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 37828.5144 - acc: 0.2524 - val_loss: 76136.9640 - val_acc: 0.2585\n",
      "Epoch 91/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 60488.6571 - acc: 0.2524 - val_loss: 24402.0551 - val_acc: 0.2585\n",
      "Epoch 92/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 61492.3466 - acc: 0.2524 - val_loss: 81498.6492 - val_acc: 0.2585\n",
      "Epoch 93/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 42209.5789 - acc: 0.2524 - val_loss: 62990.8307 - val_acc: 0.2585\n",
      "Epoch 94/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 49226.7782 - acc: 0.2524 - val_loss: 8081.2042 - val_acc: 0.2585\n",
      "Epoch 95/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 41210.0657 - acc: 0.2524 - val_loss: 30546.7587 - val_acc: 0.2585\n",
      "Epoch 96/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 48259.7305 - acc: 0.2524 - val_loss: 65147.4420 - val_acc: 0.2585\n",
      "Epoch 97/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 60638.0311 - acc: 0.2524 - val_loss: 13264.9677 - val_acc: 0.2585\n",
      "Epoch 98/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 40238.1843 - acc: 0.2524 - val_loss: 114471.9893 - val_acc: 0.2585\n",
      "Epoch 99/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 44332.8300 - acc: 0.2524 - val_loss: 3437.4208 - val_acc: 0.2585\n",
      "Epoch 100/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 50202.7618 - acc: 0.2524 - val_loss: 46368.6623 - val_acc: 0.2585\n",
      "Epoch 101/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 42815.9354 - acc: 0.2524 - val_loss: 14204.4321 - val_acc: 0.2585\n",
      "Epoch 102/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 43756.9128 - acc: 0.2524 - val_loss: 8952.5448 - val_acc: 0.2585\n",
      "Epoch 103/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 42943.1936 - acc: 0.2524 - val_loss: 122202.6781 - val_acc: 0.2585\n",
      "Epoch 104/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 33375.1497 - acc: 0.2524 - val_loss: 32758.6290 - val_acc: 0.2585\n",
      "Epoch 105/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 47922.0248 - acc: 0.2524 - val_loss: 75503.4897 - val_acc: 0.2585\n",
      "Epoch 106/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 46546.2769 - acc: 0.2524 - val_loss: 10637.5276 - val_acc: 0.2585\n",
      "Epoch 107/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 45840.0963 - acc: 0.2524 - val_loss: 62586.4676 - val_acc: 0.2585\n",
      "Epoch 108/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 61250.0686 - acc: 0.2524 - val_loss: 49516.3741 - val_acc: 0.2585\n",
      "Epoch 109/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 50223.1521 - acc: 0.2524 - val_loss: 24824.2786 - val_acc: 0.2585\n",
      "Epoch 110/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 55755.1064 - acc: 0.2524 - val_loss: 21175.6127 - val_acc: 0.2585\n",
      "Epoch 111/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 47744.7522 - acc: 0.2524 - val_loss: 33497.7789 - val_acc: 0.2585\n",
      "Epoch 112/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 40998.4576 - acc: 0.2524 - val_loss: 79040.8702 - val_acc: 0.2585\n",
      "Epoch 113/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 52583.9500 - acc: 0.2524 - val_loss: 115147.7648 - val_acc: 0.2585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 47254.3167 - acc: 0.2524 - val_loss: 47013.1566 - val_acc: 0.2585\n",
      "Epoch 115/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 42703.7688 - acc: 0.2524 - val_loss: 47330.0359 - val_acc: 0.2585\n",
      "Epoch 116/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 39682.3140 - acc: 0.2524 - val_loss: 22768.9244 - val_acc: 0.2585\n",
      "Epoch 117/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 43630.5796 - acc: 0.2524 - val_loss: 139215.0975 - val_acc: 0.2585\n",
      "Epoch 118/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 53786.8731 - acc: 0.2524 - val_loss: 28477.8047 - val_acc: 0.2585\n",
      "Epoch 119/200\n",
      "4236/4236 [==============================] - 6s 1ms/step - loss: 50028.5508 - acc: 0.2524 - val_loss: 118510.4411 - val_acc: 0.2585\n",
      "Epoch 120/200\n",
      "4236/4236 [==============================] - 6s 1ms/step - loss: 39313.5859 - acc: 0.2524 - val_loss: 33751.3921 - val_acc: 0.2585\n",
      "Epoch 121/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 45432.4560 - acc: 0.2524 - val_loss: 23454.7359 - val_acc: 0.2585\n",
      "Epoch 122/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 54107.1296 - acc: 0.2524 - val_loss: 57573.9238 - val_acc: 0.2585\n",
      "Epoch 123/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 46384.8212 - acc: 0.2524 - val_loss: 42247.3356 - val_acc: 0.2585\n",
      "Epoch 124/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 43946.2364 - acc: 0.2524 - val_loss: 98632.4451 - val_acc: 0.2585\n",
      "Epoch 125/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 47019.4534 - acc: 0.2524 - val_loss: 9749.8985 - val_acc: 0.2585\n",
      "Epoch 126/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 40594.5040 - acc: 0.2524 - val_loss: 48916.5655 - val_acc: 0.2585\n",
      "Epoch 127/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 37881.1052 - acc: 0.2524 - val_loss: 68633.5638 - val_acc: 0.2585\n",
      "Epoch 128/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 65724.8669 - acc: 0.2524 - val_loss: 45253.5347 - val_acc: 0.2585\n",
      "Epoch 129/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 49659.4612 - acc: 0.2524 - val_loss: 8719.8229 - val_acc: 0.2585\n",
      "Epoch 130/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 34036.5142 - acc: 0.2524 - val_loss: 5995.7930 - val_acc: 0.2585\n",
      "Epoch 131/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 40013.0034 - acc: 0.2524 - val_loss: 25386.3457 - val_acc: 0.2585\n",
      "Epoch 132/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 47162.9552 - acc: 0.2524 - val_loss: 36503.0370 - val_acc: 0.2585\n",
      "Epoch 133/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 40234.0390 - acc: 0.2524 - val_loss: 28223.0282 - val_acc: 0.2585\n",
      "Epoch 134/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 48420.7687 - acc: 0.2524 - val_loss: 14978.0826 - val_acc: 0.2585\n",
      "Epoch 135/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 45683.6986 - acc: 0.2524 - val_loss: 23386.6538 - val_acc: 0.2585\n",
      "Epoch 136/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 36189.1585 - acc: 0.2524 - val_loss: 61545.7676 - val_acc: 0.2585\n",
      "Epoch 137/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 42548.0204 - acc: 0.2524 - val_loss: 95306.0745 - val_acc: 0.2585\n",
      "Epoch 138/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 38563.9755 - acc: 0.2524 - val_loss: 60690.1246 - val_acc: 0.2585\n",
      "Epoch 139/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 32748.2562 - acc: 0.2524 - val_loss: 31814.5799 - val_acc: 0.2585\n",
      "Epoch 140/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 44805.4185 - acc: 0.2524 - val_loss: 1927.9414 - val_acc: 0.2585\n",
      "Epoch 141/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 33340.2612 - acc: 0.2524 - val_loss: 162777.3967 - val_acc: 0.2585\n",
      "Epoch 142/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 49742.1410 - acc: 0.2524 - val_loss: 38881.2019 - val_acc: 0.2585\n",
      "Epoch 143/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 42833.9842 - acc: 0.2524 - val_loss: 11115.0994 - val_acc: 0.2585\n",
      "Epoch 144/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 45893.2955 - acc: 0.2524 - val_loss: 6051.6106 - val_acc: 0.2585\n",
      "Epoch 145/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 48249.6057 - acc: 0.2524 - val_loss: 9294.4549 - val_acc: 0.2585\n",
      "Epoch 146/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 49579.1396 - acc: 0.2524 - val_loss: 62696.4207 - val_acc: 0.2585\n",
      "Epoch 147/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 42397.7752 - acc: 0.2524 - val_loss: 28636.5287 - val_acc: 0.2585\n",
      "Epoch 148/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 47293.6129 - acc: 0.2524 - val_loss: 6201.0991 - val_acc: 0.2585\n",
      "Epoch 149/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 49974.2865 - acc: 0.2524 - val_loss: 12896.6080 - val_acc: 0.2585\n",
      "Epoch 150/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 40957.5572 - acc: 0.2524 - val_loss: 21394.5613 - val_acc: 0.2585\n",
      "Epoch 151/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 37309.7844 - acc: 0.2524 - val_loss: 6138.0600 - val_acc: 0.2585\n",
      "Epoch 152/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 45126.2277 - acc: 0.2524 - val_loss: 36242.7898 - val_acc: 0.2585\n",
      "Epoch 153/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 50994.4544 - acc: 0.2524 - val_loss: 19095.1056 - val_acc: 0.2585\n",
      "Epoch 154/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 43015.5260 - acc: 0.2524 - val_loss: 57042.6165 - val_acc: 0.2585\n",
      "Epoch 155/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 40746.3092 - acc: 0.2524 - val_loss: 42330.9276 - val_acc: 0.2585\n",
      "Epoch 156/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 52967.2488 - acc: 0.2524 - val_loss: 97752.2163 - val_acc: 0.2585\n",
      "Epoch 157/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 35012.2202 - acc: 0.2524 - val_loss: 109015.1360 - val_acc: 0.2585\n",
      "Epoch 158/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 40608.8670 - acc: 0.2524 - val_loss: 53060.1052 - val_acc: 0.2585\n",
      "Epoch 159/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 35529.9856 - acc: 0.2524 - val_loss: 27009.7825 - val_acc: 0.2585\n",
      "Epoch 160/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 39306.2239 - acc: 0.2524 - val_loss: 56110.6695 - val_acc: 0.2585\n",
      "Epoch 161/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 46582.4419 - acc: 0.2524 - val_loss: 64746.3089 - val_acc: 0.2585\n",
      "Epoch 162/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 53219.1571 - acc: 0.2524 - val_loss: 42168.2836 - val_acc: 0.2585\n",
      "Epoch 163/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 35189.7861 - acc: 0.2524 - val_loss: 68290.6245 - val_acc: 0.2585\n",
      "Epoch 164/200\n",
      "4236/4236 [==============================] - 6s 1ms/step - loss: 40103.2711 - acc: 0.2524 - val_loss: 26221.6643 - val_acc: 0.2585\n",
      "Epoch 165/200\n",
      "4236/4236 [==============================] - 6s 2ms/step - loss: 39236.5038 - acc: 0.2524 - val_loss: 77560.9015 - val_acc: 0.2585\n",
      "Epoch 166/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 39691.2409 - acc: 0.2524 - val_loss: 15697.9457 - val_acc: 0.2585\n",
      "Epoch 167/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 39284.2136 - acc: 0.2524 - val_loss: 63310.4981 - val_acc: 0.2585\n",
      "Epoch 168/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 34882.2061 - acc: 0.2524 - val_loss: 18067.2866 - val_acc: 0.2585\n",
      "Epoch 169/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 35169.8052 - acc: 0.2524 - val_loss: 4021.2811 - val_acc: 0.2585\n",
      "Epoch 170/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4236/4236 [==============================] - 7s 2ms/step - loss: 42012.0928 - acc: 0.2524 - val_loss: 48172.0580 - val_acc: 0.2585\n",
      "Epoch 171/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 43608.4190 - acc: 0.2524 - val_loss: 82503.2768 - val_acc: 0.2585\n",
      "Epoch 172/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 40660.2525 - acc: 0.2524 - val_loss: 21129.6058 - val_acc: 0.2585\n",
      "Epoch 173/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 56635.8135 - acc: 0.2524 - val_loss: 19661.1189 - val_acc: 0.2585\n",
      "Epoch 174/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 34798.6593 - acc: 0.2524 - val_loss: 55214.4593 - val_acc: 0.2585\n",
      "Epoch 175/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 50715.1416 - acc: 0.2524 - val_loss: 55572.4556 - val_acc: 0.2585\n",
      "Epoch 176/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 44965.2824 - acc: 0.2524 - val_loss: 36278.0062 - val_acc: 0.2585\n",
      "Epoch 177/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 42550.2041 - acc: 0.2524 - val_loss: 7938.9075 - val_acc: 0.2585\n",
      "Epoch 178/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 50623.2682 - acc: 0.2524 - val_loss: 83522.8198 - val_acc: 0.2585\n",
      "Epoch 179/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 34933.2674 - acc: 0.2524 - val_loss: 3419.7457 - val_acc: 0.2585\n",
      "Epoch 180/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 49892.2906 - acc: 0.2524 - val_loss: 16601.1528 - val_acc: 0.2585\n",
      "Epoch 181/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 43899.7839 - acc: 0.2524 - val_loss: 32448.4873 - val_acc: 0.2585\n",
      "Epoch 182/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 45473.8092 - acc: 0.2524 - val_loss: 43408.1003 - val_acc: 0.2585\n",
      "Epoch 183/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 45324.2819 - acc: 0.2524 - val_loss: 10605.2254 - val_acc: 0.2585\n",
      "Epoch 184/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 34507.9860 - acc: 0.2524 - val_loss: 78234.8724 - val_acc: 0.2585\n",
      "Epoch 185/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 40578.4716 - acc: 0.2524 - val_loss: 22260.8252 - val_acc: 0.2585\n",
      "Epoch 186/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 31343.2447 - acc: 0.2524 - val_loss: 42013.4875 - val_acc: 0.2585\n",
      "Epoch 187/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 44399.2023 - acc: 0.2524 - val_loss: 2818.6269 - val_acc: 0.2585\n",
      "Epoch 188/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 44297.6308 - acc: 0.2524 - val_loss: 14643.7397 - val_acc: 0.2585\n",
      "Epoch 189/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 43721.4166 - acc: 0.2524 - val_loss: 52058.3111 - val_acc: 0.2585\n",
      "Epoch 190/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 39503.8618 - acc: 0.2524 - val_loss: 22221.0484 - val_acc: 0.2585\n",
      "Epoch 191/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 42922.8640 - acc: 0.2524 - val_loss: 66837.7868 - val_acc: 0.2585\n",
      "Epoch 192/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 32946.9481 - acc: 0.2524 - val_loss: 79515.7480 - val_acc: 0.2585\n",
      "Epoch 193/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 43750.5000 - acc: 0.2524 - val_loss: 1793.8100 - val_acc: 0.2585\n",
      "Epoch 194/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 46562.6007 - acc: 0.2524 - val_loss: 59636.2858 - val_acc: 0.2585\n",
      "Epoch 195/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 39263.6414 - acc: 0.2524 - val_loss: 13532.1381 - val_acc: 0.2585\n",
      "Epoch 196/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 34018.0944 - acc: 0.2524 - val_loss: 2595.6205 - val_acc: 0.2585\n",
      "Epoch 197/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 43763.9382 - acc: 0.2524 - val_loss: 59807.4160 - val_acc: 0.2585\n",
      "Epoch 198/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 32317.6280 - acc: 0.2524 - val_loss: 157614.3039 - val_acc: 0.2585\n",
      "Epoch 199/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 51499.6742 - acc: 0.2524 - val_loss: 13651.2205 - val_acc: 0.2585\n",
      "Epoch 200/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 43020.1153 - acc: 0.2524 - val_loss: 169898.6000 - val_acc: 0.2585\n"
     ]
    }
   ],
   "source": [
    "model, history, loss, accuracy = fit(model, trainX, testX, trainY, testY, regress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.7e+05\n",
      "Test accuracy: 0.258\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(history, loss, accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(np.expand_dims(x.reshape(x.shape[0], w_rh, h_rh), axis = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model test MSE: 0.17908436245652246\n"
     ]
    }
   ],
   "source": [
    "print('Model test MSE: {}'.format(mean_squared_error(y_pred, y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_pred > 0.1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Not good - no hotness predicted. Now try classification*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.106232294617563 % of the songs are classified as hot, while the rest is not\n"
     ]
    }
   ],
   "source": [
    "y = convert_y_to_categorical(y_df)\n",
    "y = y.astype('int')\n",
    "\n",
    "print(\"{} % of the songs are classified as hot, while the rest is not\".format(y.mean()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 12, 19, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 12, 19, 16)        160       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 12, 19, 16)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 12, 19, 16)        64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 9, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 6, 9, 32)          4640      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 6, 9, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 6, 9, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 3, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 4, 64)          18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 3, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 3, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 1, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                2064      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 68        \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 25,945\n",
      "Trainable params: 25,689\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "regress = False\n",
    "model, trainX, testX, trainY, testY = getModelAndDs(x, y, w_rh, h_rh, regress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4236 samples, validate on 1412 samples\n",
      "Epoch 1/200\n",
      "4236/4236 [==============================] - 8s 2ms/step - loss: 84848068.4858 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 2/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 2528774.1201 - acc: 0.7517 - val_loss: 438702.9842 - val_acc: 0.7408\n",
      "Epoch 3/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 740683.6003 - acc: 0.7517 - val_loss: 119199.3040 - val_acc: 0.7408\n",
      "Epoch 4/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 341962.8794 - acc: 0.7517 - val_loss: 31353.8175 - val_acc: 0.7408\n",
      "Epoch 5/200\n",
      "4236/4236 [==============================] - 6s 2ms/step - loss: 211981.9188 - acc: 0.7517 - val_loss: 21750.0394 - val_acc: 0.7408\n",
      "Epoch 6/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 141877.8748 - acc: 0.7517 - val_loss: 4684.1164 - val_acc: 0.7408\n",
      "Epoch 7/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 90183.7225 - acc: 0.7517 - val_loss: 1924.5734 - val_acc: 0.7408\n",
      "Epoch 8/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 84584.2009 - acc: 0.7517 - val_loss: 1394.7802 - val_acc: 0.7408\n",
      "Epoch 9/200\n",
      "4236/4236 [==============================] - 6s 1ms/step - loss: 40997.2566 - acc: 0.7517 - val_loss: 666.2907 - val_acc: 0.7408\n",
      "Epoch 10/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 74740.6951 - acc: 0.7517 - val_loss: 842.7829 - val_acc: 0.7408\n",
      "Epoch 11/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 32273.2510 - acc: 0.7517 - val_loss: 842.9095 - val_acc: 0.7408\n",
      "Epoch 12/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 26003.6084 - acc: 0.7517 - val_loss: 180.5255 - val_acc: 0.7408\n",
      "Epoch 13/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 18334.7732 - acc: 0.7517 - val_loss: 335.0037 - val_acc: 0.7408\n",
      "Epoch 14/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 22476.0291 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 15/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 6797.9750 - acc: 0.7517 - val_loss: 69.9488 - val_acc: 0.7408\n",
      "Epoch 16/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 6457.6047 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 17/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 25780.3874 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 18/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 3459.4889 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 19/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 10008.6271 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 20/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 1826.3837 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 21/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 2695.7869 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 22/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 1050.2004 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 23/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 1591.1451 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 24/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 363.0235 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 25/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 1011.0834 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 26/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 1522.7532 - acc: 0.7517 - val_loss: 12720.3381 - val_acc: 0.7408\n",
      "Epoch 27/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 559.5317 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 28/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 301.9063 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 29/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 369.3906 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 30/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 225.0787 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 31/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 147.4492 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 32/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 351.6683 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 33/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 332.0745 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 34/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 5422.9035 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 35/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 38.3218 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 36/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 136.6990 - acc: 0.7517 - val_loss: 288877647.2295 - val_acc: 0.7408\n",
      "Epoch 37/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 86.1068 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 38/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 4101.5687 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 39/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 120.6370 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 40/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 33.9806 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 41/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 33.7839 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 42/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 54.0461 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 43/200\n",
      "4236/4236 [==============================] - 6s 2ms/step - loss: 26.6499 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 44/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 67.7090 - acc: 0.7517 - val_loss: 26465984.9972 - val_acc: 0.7408\n",
      "Epoch 45/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 179.2426 - acc: 0.7517 - val_loss: 282414367.0028 - val_acc: 0.7408\n",
      "Epoch 46/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 25.5172 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 47/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 29.0209 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 48/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 58.3377 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 49/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 53.0048 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 50/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 25.6227 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 51/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 41.8255 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 52/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 25.9253 - acc: 0.7517 - val_loss: 3322977.0680 - val_acc: 0.7408\n",
      "Epoch 53/200\n",
      "4236/4236 [==============================] - 6s 2ms/step - loss: 25.7705 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 54/200\n",
      "4236/4236 [==============================] - 5s 1ms/step - loss: 26.2559 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 55/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 26.5444 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 56/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 25.3694 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 57/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 39.8907 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 58/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 25.3202 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4236/4236 [==============================] - 7s 2ms/step - loss: 34.2342 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 60/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 25.1725 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 61/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 25.1584 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 62/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 25.1936 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 63/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 31.4200 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 64/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 25.0669 - acc: 0.7517 - val_loss: 271738436.8952 - val_acc: 0.7408\n",
      "Epoch 65/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 31.9054 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 66/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8559 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 67/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 37.0343 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 68/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 69/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8629 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 70/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 29.2601 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 71/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 26.6147 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 72/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 27.2409 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 73/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 26.2348 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 74/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 25.6509 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 75/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 76/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 77/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 78/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 79/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 25.0528 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 80/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.9614 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 81/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 82/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 83/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 84/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 25.3554 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 85/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.9825 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 86/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 87/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.9051 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 88/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 89/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 90/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.9403 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 91/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 92/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8840 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 93/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 25.0177 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 94/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 95/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8770 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 96/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8910 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 97/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 98/200\n",
      "4236/4236 [==============================] - 6s 1ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 99/200\n",
      "4236/4236 [==============================] - 6s 1ms/step - loss: 67.8638 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 100/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 101/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8418 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 102/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 103/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8699 - acc: 0.7517 - val_loss: 220428708.8045 - val_acc: 0.7408\n",
      "Epoch 104/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8418 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 105/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 106/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 107/200\n",
      "4236/4236 [==============================] - 6s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 108/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8488 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 109/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 110/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8629 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 111/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8488 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 112/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8910 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 113/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 114/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 115/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 116/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8699 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 117/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 118/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 119/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 120/200\n",
      "4236/4236 [==============================] - 6s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 121/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 122/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 123/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 124/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 125/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 126/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8488 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 127/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 128/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 129/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 130/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 131/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 132/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8488 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 133/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 134/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 135/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 136/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 137/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 138/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 139/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8488 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 140/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 141/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 142/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 143/200\n",
      "4236/4236 [==============================] - 6s 1ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 144/200\n",
      "4236/4236 [==============================] - 6s 1ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 145/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8418 - acc: 0.7517 - val_loss: 207539101.1898 - val_acc: 0.7408\n",
      "Epoch 146/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 147/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 148/200\n",
      "4236/4236 [==============================] - 6s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 149/200\n",
      "4236/4236 [==============================] - 5s 1ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 150/200\n",
      "4236/4236 [==============================] - 6s 1ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 151/200\n",
      "4236/4236 [==============================] - 5s 1ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 152/200\n",
      "4236/4236 [==============================] - 6s 1ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 153/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 154/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 155/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 156/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 157/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 158/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 159/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 160/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 34.2412 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 161/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 162/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 163/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 31.7436 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 164/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 165/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 166/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 167/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 168/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 169/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 170/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 171/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 172/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 173/200\n",
      "4236/4236 [==============================] - 6s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 174/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 175/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 176/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 178/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 179/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 180/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 30.0973 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 181/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 182/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 183/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 28.7113 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 184/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 185/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 186/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 187/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 188/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 189/200\n",
      "4236/4236 [==============================] - 6s 1ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 190/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 191/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 192/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 193/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 194/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 195/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 27.8811 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 196/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 197/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 198/200\n",
      "4236/4236 [==============================] - 6s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 199/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n",
      "Epoch 200/200\n",
      "4236/4236 [==============================] - 7s 2ms/step - loss: 24.8347 - acc: 0.7517 - val_loss: 25.9207 - val_acc: 0.7408\n"
     ]
    }
   ],
   "source": [
    "model, history, loss, accuracy = fit(model, trainX, testX, trainY, testY, regress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 25.9\n",
      "Test accuracy: 0.741\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(history, loss, accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(np.expand_dims(x.reshape(x.shape[0], w_rh, h_rh), axis = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy - classification 0.7489376770538244\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy - classification', accuracy_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*~75% accuracy - much better than for regression but still no improvement over baseline*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
